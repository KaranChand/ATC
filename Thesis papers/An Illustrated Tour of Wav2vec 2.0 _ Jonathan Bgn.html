<!DOCTYPE html>
<!-- saved from url=(0061)https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>An Illustrated Tour of Wav2vec 2.0 | Jonathan Bgn</title>
<meta name="generator" content="Jekyll v3.9.0">
<meta property="og:title" content="An Illustrated Tour of Wav2vec 2.0">
<meta name="author" content="Jonathan Boigne">
<meta property="og:locale" content="en_US">
<meta name="description" content="Self-supervised learning of speech representations explained visually.">
<meta property="og:description" content="Self-supervised learning of speech representations explained visually.">
<link rel="canonical" href="https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html">
<meta property="og:url" content="https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html">
<meta property="og:site_name" content="Jonathan Bgn">
<meta property="og:image" content="https://jonathanbgn.com/assets/images/illustrated-wav2vec/wav2vec2_architecture_pretraining.png">
<meta property="og:type" content="article">
<meta property="article:published_time" content="2021-09-30T14:00:00+00:00">
<meta name="twitter:card" content="summary_large_image">
<meta property="twitter:image" content="https://jonathanbgn.com/assets/images/illustrated-wav2vec/wav2vec2_architecture_pretraining.png">
<meta property="twitter:title" content="An Illustrated Tour of Wav2vec 2.0">
<script async="" src="./An Illustrated Tour of Wav2vec 2.0 _ Jonathan Bgn_files/analytics.js.download"></script><script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Jonathan Boigne"},"dateModified":"2021-09-30T14:00:00+00:00","datePublished":"2021-09-30T14:00:00+00:00","description":"Self-supervised learning of speech representations explained visually.","headline":"An Illustrated Tour of Wav2vec 2.0","image":"https://jonathanbgn.com/assets/images/illustrated-wav2vec/wav2vec2_architecture_pretraining.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html"},"url":"https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="icon" type="image/svg+xml" href="https://jonathanbgn.com/assets/favicon.svg">
  <link rel="stylesheet" href="./An Illustrated Tour of Wav2vec 2.0 _ Jonathan Bgn_files/main.css">

  <link rel="preconnect" href="https://fonts.googleapis.com/"> 
  <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin=""> 
  <link href="./An Illustrated Tour of Wav2vec 2.0 _ Jonathan Bgn_files/css2" rel="stylesheet"><link type="application/atom+xml" rel="alternate" href="https://jonathanbgn.com/feed.xml" title="Jonathan Bgn"><!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="./An Illustrated Tour of Wav2vec 2.0 _ Jonathan Bgn_files/js"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-Q73H8CQX17');
</script>

<!-- Previous analytics.js used by Universal Analytics -->
<script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-156522905-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
<script type="text/javascript" id="MathJax-script" async="" src="./An Illustrated Tour of Wav2vec 2.0 _ Jonathan Bgn_files/tex-chtml.js.download"></script>
<style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}
</style></head>
<body class="vsc-initialized"><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="https://jonathanbgn.com/">Jonathan Bgn</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger">
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"></path>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="https://jonathanbgn.com/projects.html">Projects</a></div>
      </nav></div>
</header><main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope="" itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">An Illustrated Tour of Wav2vec 2.0</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2021-09-30T14:00:00+00:00" itemprop="datePublished">Sep 30, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Transformer-based neural networks have been revolutionizing the natural language processing field, but are only starting to become popular in the speech processing community. <strong>Wav2vec 2.0</strong> is set to change that. Its architecture is based on the Transformer’s encoder, with a training objective similar to <strong>BERT’s masked language modeling</strong> objective, but adapted for speech.</p>

<p>This new method allows for efficient <a href="https://en.wikipedia.org/wiki/Semi-supervised_learning">semi-supervised training</a>: first, pre-train the model on a large quantity of unlabeled speech, then fine-tune on a smaller labeled dataset. In <a href="https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html">wav2vec 2.0’s original paper</a>, the authors demonstrated that fine-tuning the model on only one hour of labeled speech data could beat the previous state-of-the-art systems trained on 100 times more labeled data.</p>

<p><img src="./An Illustrated Tour of Wav2vec 2.0 _ Jonathan Bgn_files/wav2vec2_architecture_pretraining.png" alt="Wav2vec 2.0 Architecture Overview"></p>

<p>Above is an overview of the wav2vec 2.0 architecture and its pre-training process. There are four important elements in this diagram: the <strong>feature encoder</strong>, <strong>context network</strong>, <strong>quantization module</strong>, and the <strong>contrastive loss</strong> (pre-training objective). We will open the hood and look in detail at each one.</p>

<h2 id="feature-encoder">Feature encoder</h2>

<p>The feature encoder’s job is to reduce the dimensionality of the audio data, converting the raw waveform into a sequence of feature vectors <em>Z<sub>0</sub>, Z<sub>1</sub>, Z<sub>2</sub>, …, Z<sub>T</sub></em> each 20 milliseconds. Its architecture is simple: a 7-layer convolutional neural network (single-dimensional) with 512 channels at each layer.</p>

<p><img src="./An Illustrated Tour of Wav2vec 2.0 _ Jonathan Bgn_files/wav2vec2_feature_encoder.png" alt="Wav2vec 2.0 Latent Feature Encoder"></p>

<p>The waveform is normalized before being sent to the network, and the kernel width and strides of the convolutional layers decrease as we get higher in the network. The feature encoder has a total receptive field of 400 samples or 25 ms of audio (audio data is encoded at a sample rate of 16 kHz).</p>

<h2 id="quantization-module">Quantization module</h2>

<p>One of the main obstacles of using Transformers for speech processing is the <strong>continuous nature of speech</strong>. Written language can be naturally discretized into words or sub-words, therefore creating a finite vocabulary of discrete units. Speech doesn’t have such natural sub-units. We could use phones as a discrete system, but then we would need humans to first label the entire dataset beforehand, so we wouldn’t be able to pre-train on unlabeled data.</p>

<p>Wav2vec 2.0 proposes to <strong>automatically learn discrete speech units</strong>, by sampling from the <a href="https://paperswithcode.com/method/gumbel-softmax">Gumbel-Softmax distribution</a>. Possible units are made of <em>codewords</em> sampled from <em>codebooks</em> (groups). <em>Codewords</em> are then concatenated to form the final speech unit. Wav2vec uses 2 groups with 320 possible words in each group, hence a theoretical maximum of 320 x 320 = 102,400 speech units.</p>

<p><img src="./An Illustrated Tour of Wav2vec 2.0 _ Jonathan Bgn_files/wav2vec2_quantization_module.png" alt="Wav2vec 2.0 Quantization Module"></p>

<p>The latent features are multiplied by the <em>quantization matrix</em> to give the logits: one score for each of the possible <em>codewords</em> in each <em>codebook</em>. The Gumbel-Softmax trick allows sampling a single codeword from each codebook, after converting these logits into probabilities. It is similar to taking the <a href="https://en.wikipedia.org/wiki/Arg_max">argmax</a> except that the operation is fully differentiable. Moreover, a small randomness effect, whose effect is controlled by a temperature argument, is introduced to the sampling process to facilitate training and codewords utilization.</p>

<h2 id="context-network">Context network</h2>

<p>The core of wav2vec 2.0 is its Transformer encoder, which takes as input the latent feature vectors and processes it through 12 Transformer blocks for the <em>BASE</em> version of the model, or 24 blocks for the <em>LARGE</em> version. To match the inner dimension of the Transformer encoder, the input sequence first needs to go through a feature projection layer to increase the dimension from 512 (output of the CNN) to 768 for <em>BASE</em> or 1,024 for <em>LARGE</em>. I will not describe further the Transformer architecture here and invite you to read <a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> if you forgot the details.</p>

<p><img src="./An Illustrated Tour of Wav2vec 2.0 _ Jonathan Bgn_files/wav2vec2_context_network_transformer.png" alt="Wav2vec 2.0 Transformer Context Network"></p>

<p>One difference from the original Transformer architecture is how positional information is added to the input. Since the self-attention operation of the Transformer doesn’t preserve the order of the input sequence, fixed pre-generated positional embeddings were added to the input vectors in the original implementation. The wav2vec model instead uses a new <a href="https://paperswithcode.com/method/grouped-convolution">grouped convolution layer</a> to learn relative positional embeddings by itself.</p>

<h2 id="pre-training--contrastive-loss">Pre-training &amp; contrastive loss</h2>

<p>The pre-training process uses a contrastive task to train on unlabeled speech data. A mask is first randomly applied in the latent space, where ~50% of the projected latent feature vectors. Masked positions are then replaced by the same trained vector <em>Z’<sub>M</sub></em> before being fed to the Transformer network.</p>

<p><img src="./An Illustrated Tour of Wav2vec 2.0 _ Jonathan Bgn_files/wav2vec2_contrastive_loss.png" alt="Wav2vec 2.0 Contrastive Loss"></p>

<p>The final context vectors then go through the last projection layer to match the dimension of the quantized speech units <em>Q<sub>t</sub></em>. For each masked position, <strong>100 negative distractors are uniformly sampled from other positions in the same sentence</strong>. The model then compares the similarity (<a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a>) between the projected context vector <em>C’<sub>t</sub></em> and the true positive target <em>Q<sub>p</sub></em> along with all negative distractors <em>Q<sub>ñ</sub></em>. The contrastive loss then encourages high similarity with the true positive target and penalizes high similarity scores with negative distractors.</p>

<h3 id="diversity-loss">Diversity loss</h3>

<p>During pre-training, another loss is added to the contrastive loss to encourage the model to use all codewords equally often. This works by maximizing the <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">entropy</a> of the Gumbel-Softmax distribution, preventing the model to always choose from a small sub-group of all available codebook entries. You can find more details in the <a href="https://proceedings.neurips.cc/paper/2020/hash/92d1e1eb1cd6f9fba3227870bb6d7f07-Abstract.html">original paper</a>.</p>

<h2 id="conclusion">Conclusion</h2>

<p>This concludes our tour of wav2vec 2.0 and its pre-training process. The resulting pre-trained model can be used for a variety of speech downstream tasks: automatic speech recognition, <a href="https://jonathanbgn.com/speech/2020/10/31/emotion-recognition-transfer-learning-wav2vec.html">emotion detection</a>, speaker recognition, language detection… In the original paper, the authors directly fine-tuned the model for speech recognition with a <a href="https://distill.pub/2017/ctc/">CTC loss</a>, adding a linear projection on top of the context network to predict a word token at each timestep.</p>

<h4 id="read-next">Read next</h4>

<p><a href="https://jonathanbgn.com/2021/10/30/hubert-visually-explained.html">HuBERT: How to Apply BERT to Speech, Visually Explained</a></p>

<p><a href="https://jonathanbgn.com/2021/06/29/illustrated-wav2vec.html">The Illustrated Wav2vec 1.0</a></p>

<p><a href="https://jonathanbgn.com/speech/2020/10/31/emotion-recognition-transfer-learning-wav2vec.html">Detecting Emotions from Voice with Very Few Training Data</a></p>

<p><a href="https://jonathanbgn.com/2020/12/31/self-supervised-learning.html">The Rise of Self-Supervised Learning</a></p>


  </div><a class="u-url" href="https://jonathanbgn.com/2021/09/30/illustrated-wav2vec-2.html" hidden=""></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Jonathan Boigne</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1"><ul class="social-media-list"><li><a href="https://www.linkedin.com/in/jonathan-boigne"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#linkedin"></use></svg> <span class="username">jonathan-boigne</span></a></li><li><a href="https://github.com/jonathanbgn"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jonathanbgn</span></a></li><li><a href="https://www.twitter.com/jonathanbgn"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jonathanbgn</span></a></li></ul>
<ul class="contact-list"><li><a class="u-email" href="mailto:hello@jonathanbgn.com">hello@jonathanbgn.com</a></li></ul>

      </div>

      <div class="footer-col footer-col-2">
      </div>

      <div class="footer-col footer-col-3">
        <p>Building stuff with machine learning and natural language processing.</p>
      </div>
    </div>

  </div>

</footer>



</body></html>